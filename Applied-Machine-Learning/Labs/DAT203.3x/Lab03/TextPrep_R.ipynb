{"cells": [{"cell_type": "markdown", "source": "### Applications of Data Science\n# Lab 3\n# Preparing Text Data with R\n\n## Overview\n\nThis lab introduces you to the basics of text mining and text data preparation. In this lab you will work with a set of 160,000 tweets, which include sentiment labels. \n\nSocial media sentiment is an important indicator of public opinion.  Determining sentiment can be valuable in a number of applications including brand awareness, product launches, and detecting political trends. \n\nRaw text is inherently messy. Machine understanding and analysis is inhibited by the presence of extraneous symbols and words that clutter the text. The exact nature of the required text cleaning depends on the application.  In this case, you will focus on text cleaning to facilitate sentiment classification.The presence of certain words determines the sentiment of the tweet. Words and symbols which are extraneous to this purpose are distractions at best, and a likely source of noise in the analysis. You will follow these steps to prepare the tweet text for analysis: \n\n- Symbols and unnecessary white space which do not convey sentiment are removed, leaving only alphabetic characters.\n- There is no difference in the sentiment conveyed by a word in upper case or lower case, so all case is set to lower. \n- Stop words are words that occur with high frequency in text, but do not have any particular meaning. Examples include word like \"the\", \"and\" and \"this\". Since these words are relatively common, yet communicate no particular sentiment, they can bias analytics.  Therefore, stopwords which do not convey sentiment are therefore removed from the tweet text.\n- stem is a root word. For example, \"go\" is the root word of conjugated verbs, \"going\", \"goes\"\u00c2\u0099, \"gone\". The meaning of these words is the same in terms of analysis. A process known as stemming is applied to transform words to their roots, before analysis. \n\n\n## What you will need\nTo complete this lab, you will need the following:\n- A web browser and Internet connection\n- An Azure ML workspace\n- The lab files for this lab\n\n\n## Load and transform the tweet data\n\nAs a first step, ensure that you have uploaded the **tweets.csv** and **stopwords.csv ** files as new datasets in your Azure Machine Learning workspace. Then use the following code to load the tweets data set and set the column names to convenient values.", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "library(tm)\nlibrary(\"AzureML\")\nws <- workspace()\ndataset <- download.datasets(ws, \"tweets.csv\")\ncolnames(dataset) <- c(\"sentiment\", \"tweets\") # Set the column names\nhead(dataset)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Examine the head of the data frame shown above, noticing the content of the two columns.\n\n- The **Sentiment** column contains a sentiment score ``{0,4}`` for negative of positive sentiment of the tweet.\n- The **Tweets** column contains the actual text of the tweet. \n\nIn order to work with the text in the tweets using the tools in the R tm package, you must convert them to a corpus object. A tm vector corpus is a vector of corpus objects. In this case, the text of each tweet is a single corpus. \n\nExecute the code in the cell below to create a tm vector corpus object. ", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "tweet.text <- Corpus(VectorSource(dataset['tweets']))\nclass(tweet.text)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Clean and Lower Case Symbols with R \nIn this exercise you will take the first steps in preparing the tweet text using custom R code. You will use the R Text Mining package **tm** to perform some basic filtering and cleaning of the tweet text. Using the tools in the tm package you will perform the following steps:\n\n- Remove numbers.\n- Remove punctuation.\n- Remove excess white space.\n- Convert to lower case.\n\nEach line in the code cell below using the **tm_map** funciton to apply the specified transformation to the text. Execute the code in the cell below to clean and lower case the tweet text. ", "metadata": {}}, {"metadata": {"collapsed": true}, "cell_type": "code", "source": "tweet.text <- tm_map(tweet.text, content_transformer(removeNumbers))\ntweet.text <- tm_map(tweet.text, content_transformer(removePunctuation))\ntweet.text <- tm_map(tweet.text, content_transformer(stripWhitespace))\ntweet.text <- tm_map(tweet.text, content_transformer(tolower))", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Next, you will create a term-document matrix (TDM) of the tweets. A TDM is a sparse matrix structure with the words (terms) in the rows and documents which may or may not contain that word in the columns. The count of word occurrence of the document is contained in the cells.\n\nThe frequency of words is simply computed by summing the values in the rows. Note that the row_sums function from the slam package is used to deal with the sparse matrix structure. A dataframe is then constructed with the words and their frequencies in descending order. \n\nExecute the code in the cell below to compute the TDM, the word frequencies and examine the head of the data frame.", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "to.WF = function(corpus){\n    require(tm)\n    ## Compute a term-document matrix and then \n    ## compute the word frequencies.\n    library(slam)\n    tdm <- TermDocumentMatrix(corpus, control = list(stopwords = FALSE))\n    tdm <- removeSparseTerms(tdm, 0.9999999)\n    freq <- row_sums(tdm, na.rm = T)   \n    ## Sort the word frequency and build a dataframe\n    ## including the cumulative frequecy of the words.\n    freq <- sort(freq, decreasing = TRUE)\n    word.freq <- data.frame(word = factor(names(freq), levels = names(freq)), \n                        freq = freq)\n    word.freq['Cum'] <- cumsum(word.freq['freq'])/sum(word.freq$freq)\n    word.freq\n}\n\nwf = to.WF(tweet.text)\nhead(wf, n = 20)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Notice that the most frequent words are in the head of this data frame. Of these 20 most frequent words, only one, 'good', is likely to convey much information on sentiment.  \n\nThe code in the cell below uses the **ggplot2** package to create a bar plot of the work frequencies computed from the TDM. Execute this code to view the bar plot. ", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "bar.TDF = function(word.freq){\n    library(ggplot2)\n    #dev.new(width=12, height=9)\n    ggplot(word.freq[1:40,], aes(word, freq)) +\n      geom_bar(stat = 'identity') +\n      ggtitle('Frequency of most common words') +\n      ylab('Frequency') +\n      theme(axis.text.x = element_text(angle = 90, hjust = 1))\n}\nbar.TDF(wf)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Examine this plot and notice the most frequent words. Many of the most frequent words are stop words, such as \"the\", \"and\", and \"you\"\u00c2\u0099, which are not likely to be helpful in determining sentiment. Also, the frequency of the words drops off fairly quickly to less than 500 out of the 160,000 tweets.\n\nAnother tool for examining the frequency of words in a corpus of documents is the cumulative distribution frequency (CDF) plot. Execute the code in the cell below to display the CDF using ggplot2. ", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "cdf.TDF = function(word.freq){\n    ggplot(word.freq[1:40,], aes(word, Cum)) +\n    geom_bar(stat = 'identity') +\n    ggtitle('Cumulative fraction of most common words') +\n    ylab('Cumulative frequency') +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n}\ncdf.TDF(wf)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "The conclusions one can draw from the second chart are largely the same as the first. The most frequent words are stop words and the frequency of words drops off rather quickly. ALso notice, that the frequency of the words becomes uniform fairly quickly. \n\nFinally, you can examine the normalized text in the processed tweets by executing the code in the cell below. ", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "head(data.frame(sentiment = dataset$sentiment, \n                text = enc2utf8(unlist(sapply(tweet.text, `[`, \"content\"))), stringsAsFactors=F))", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Examine this text. Notice that all text is lower case and there are no numbers, punctuation or special characters. \n\n****\n\nYou will now Examine the head of the resulting word frequency data frame to determine the following:\n\n- What is the percentage of all words for these first 20 words? \n- Of these 20 words, how many are likely to contibute sentiment information? \n- Are these 20 words different from the words seen for the raw text? \n\nTo perform this exercise, apply the **head** function, with the n = 20 argument, to the **wf** data frame. \n****", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "head(wf, n = 20)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Remove stop words\n\nIn the previous section you removed extraneous characters and whitespace from the tweet text. The results show that the most frequent words do not communicate much sentiment information. These frequent words, which are largely extraneous, are known as stop words and should be removed from the text before further analysis. In this exercise you will use custom R code to remove stop words from the tweet text.\n\nAs a first step you will load the list of stop words, and examine the first 100 by executing the code in the cell below. ", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "stop.words <- download.datasets(ws, \"stopwords.csv\")\nstop.words = unique(stop.words)\nstop.words[1:100,]", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Examine the stop word list and notice the following:\n\n- These words are generally common in English language text.\n- None of these words seem likely to indicate any particular sentiment. \n- Some of these words, like 'aww', are specialized to this application of analyzing tweets. \n\nThe code in the cell below applies the **removeWords** operation to the tweet text. Execute the code in the cell to remove the stop words from the tweets and plot the word frequency.", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "tweet.text <- tm_map(tweet.text, removeWords, stop.words[, 'words'])\nwf = to.WF(tweet.text)\nbar.TDF(wf)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "The distribution of word frequency is not quite different. Note that many of the most frequent words are now likely to convey some sentiment, such as \"\u00c2\u0098good\", \"like\", and \"love\". Evidently, removing stop words has had the desired effect.\n\nNext, execute the code in the cell below to display the CDF of the tweets with the stop words removed. ", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "cdf.TDF(wf)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "As before, this chart shows a number of frequent words which are likely to convey sentiment. However, note that these 40 most frequent words only make up about 15% or the total. \n\n****\n\nYou will now examine the head of the resulting word frequency data frame to determine the following:\n\n- What is the percentage of all words for these first 20 words? \n- Of these 20 words, how many are likely to contribute sentiment information? \n- Are these 20 words different from the words seen for the normalized text? \n\nTo perform this exercise, apply the **head** function, with the **n = 20** argument, to the **wf** data frame. \n****", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "head(wf, n = 20)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "## Stem the Words\n\nYou have cleaned the tweet text and removed stop words. There is one last data preparation step required, stemming the words. Stemming is a process of reducing words to their stems or roots. For example, conjugated verbs such as \"goes\", \"going\", and \"gone\" are stemmed to the word \"go\".  Both Python and R offer a choice of stemmers. Depending on this choice, the results can be more or less suitable for the application. In this case, you will use the popular Porter stemmer. \n\nThe Porter stemmer is in the R **SnowBallC** library. Execute the code in the cell below to load and apply the Porter stemmer to the tweet text and lot the bar chart of the word frequency.", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "library(SnowballC) ## For stemming words\ntweet.text <- tm_map(tweet.text, stemDocument)\nwf = to.WF(tweet.text)\nbar.TDF(wf)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "To display the CDF plot of the word frequency, execute the code in the cell below. ", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "cdf.TDF(wf)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Compare these two charts using the stemmed words to the charts created with just stop word filtering and notice the differences. These differences are quite noticeable. For example, some words like \"good\" and \"like\" have moved higher in the order of most frequent words, while some other words like \"going\" have moved down. \n\nFinally, to examine a sample of the prepared tweet text, . ", "metadata": {}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "head(data.frame(sentiment = dataset$sentiment, \n                text = enc2utf8(unlist(sapply(tweet.text, `[`, \"content\"))), stringsAsFactors=F))", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "****\n\nYou will now examine the head of the resulting word frequency data frame to determine the following:\n\n- Have any of the words in the list been stemmed? \n- Has the stemming changed the frequency of these words?\n\nTo perform this exercise, apply the **head** function, with the **n = 20** argument, to the **wf** data frame.\n****", "metadata": {"collapsed": true}}, {"metadata": {"collapsed": false}, "cell_type": "code", "source": "head(wf, n=20)", "outputs": [], "execution_count": null}], "nbformat": 4, "nbformat_minor": 0, "metadata": {"kernelspec": {"display_name": "R", "language": "R", "name": "r"}, "language_info": {"mimetype": "text/x-r-source", "codemirror_mode": "r", "pygments_lexer": "r", "file_extension": ".r", "version": "3.1.1", "name": "R"}}}